{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wn_ic\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_of_training_file(training_file_name):\n",
    "    xml_file = ET.parse(training_file_name)\n",
    "    return xml_file.getroot()\n",
    "\n",
    "def get_node_text(node):\n",
    "    if node.text:\n",
    "        result = node.text\n",
    "    else:\n",
    "        result = ''\n",
    "    for child in node:\n",
    "        if child.tail is not None:\n",
    "            result += child.tail\n",
    "    return result\n",
    "    \n",
    "def get_noun_sentences_to_parse(root):\n",
    "    sentences = {}\n",
    "    for lexelt in root:\n",
    "        target = lexelt.attrib['item']\n",
    "        target = target.split('.')\n",
    "        target_tags = target[1:len(target)]\n",
    "        if 'n' in target_tags:\n",
    "            target_word = target[0]\n",
    "            for instance in lexelt:\n",
    "                instance_id = instance.attrib['id']\n",
    "                for context in instance:\n",
    "                    sentence_text = []\n",
    "                    if context.text:\n",
    "                        sentence_text.append(context.text)\n",
    "                    else: \n",
    "                        sentence_text.append('')\n",
    "                    for child in context:\n",
    "                        sentence_text.append(child.tail)\n",
    "                        sentence_text.append(child.text)\n",
    "                sentences[instance_id] = sentence_text\n",
    "    return sentences\n",
    "        \n",
    "def get_pos_for_sentences(noun_sentences):\n",
    "    parsed_sentences = {}\n",
    "    for key in noun_sentences:\n",
    "        sentence_to_find = noun_sentences[key][0] + noun_sentences[key][2] + noun_sentences[key][1]\n",
    "        parsed_sentences[key] = get_sentence_from_parsed(sentence_to_find)\n",
    "    return parsed_sentences\n",
    "  \n",
    "def get_sentence_from_parsed(sentence_to_find):                \n",
    "    tokenized = nltk.word_tokenize(sentence_to_find)\n",
    "    return nltk.pos_tag(tokenized)\n",
    "    \n",
    "                \n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_target_word(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemword = stemmer.stem(word)\n",
    "    if wn.synsets(stemword) is None:\n",
    "        stemword = word\n",
    "    return stemword\n",
    "\n",
    "def sentences_to_lowercase(sentence):\n",
    "    return ([segment.lower() for segment in sentence])\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    lowercase_sentence = sentences_to_lowercase(sentence)\n",
    "    stemmed_target_word = stem_target_word(lowercase_sentence[2])\n",
    "    return lowercase_sentence,stemmed_target_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence_to_array(sentence):\n",
    "    return (sentence[0] + sentence[2] + sentence[1]).split()\n",
    "\n",
    "def perform_lesk(sentence,word):\n",
    "    return lesk(sentence,word,'n')\n",
    "\n",
    "def create_lesk_replacement_dictionary(noun_sentences):\n",
    "    replacement_dictionary = {}\n",
    "    # For every sentence\n",
    "    for sentence in noun_sentences:\n",
    "        # place target word to lowercase.\n",
    "        preprocessed_sentence,target_word = preprocess_sentence(noun_sentences[sentence])     \n",
    "        # Get the context synset first using preprocessing\n",
    "        context_synset = perform_lesk(parse_sentence_to_array(preprocessed_sentence),target_word)\n",
    "        \n",
    "        if context_synset is None:\n",
    "            context_synset = perform_lesk(parse_sentence_to_array(preprocessed_sentence),noun_sentences[sentence][2])\n",
    "            \n",
    "        for lemma in context_synset.lemmas():\n",
    "            if target_word != lemma.name():\n",
    "                replacement_word = lemma.name()\n",
    "                if '_' in replacement_word:\n",
    "                    split_word = ''\n",
    "                    for word in replacement_word.split('_'):\n",
    "                        split_word += ' ' + word\n",
    "                    replacement_word = split_word[1:]\n",
    "                replacement_dictionary[sentence] = replacement_word\n",
    "                break;\n",
    "    return replacement_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph sense prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pos_tags(pos_tags):\n",
    "    processed_tags = {}\n",
    "    for key in pos_tags:\n",
    "        current_sentence = pos_tags[key]\n",
    "        text_sentence = [(sentence[0].lower(),sentence[1]) for sentence in current_sentence]\n",
    "        stopwords_removed =  [(w[0],w[1]) for w in text_sentence if not w[0] in stopwords.words('english')]\n",
    "        punctuation = ['!',',','.','?',')','(']\n",
    "        punctuation_removed = [(w[0],w[1]) for w in stopwords_removed if not w[0] in punctuation]\n",
    "        processed_tags[key] = punctuation_removed\n",
    "    return processed_tags\n",
    "        \n",
    "def get_synsets(pos_tuple):\n",
    "    wn_pos_code = wordnet_pos_code(pos_tuple[1])\n",
    "    if wn_pos_code is not None:\n",
    "        return wn.synsets(pos_tuple[0],wn_pos_code)\n",
    "    \n",
    "def create_graph(sentence):\n",
    "    graph = {}\n",
    "    for pos_tuple in sentence:\n",
    "        synsets = get_synsets(pos_tuple)\n",
    "        if synsets is not None:\n",
    "            for s in synsets:\n",
    "                graph[s] = [];\n",
    "    return graph      \n",
    "\n",
    "def build_synset_tree(level,synset,tree,max_level=2):\n",
    "    if synset not in tree and level != 0:\n",
    "        tree.append(synset)\n",
    "    if level <= max_level:\n",
    "        # Get Hyponyms\n",
    "        for s in synset.hyponyms():\n",
    "            build_synset_tree(level+1,s,tree)\n",
    "        # Get Norminalisations\n",
    "        for l in synset.lemmas():\n",
    "            related_forms = l.derivationally_related_forms()\n",
    "            for rf in related_forms:\n",
    "                build_synset_tree(level+1,rf.synset(),tree) \n",
    "        return tree\n",
    "    else:\n",
    "        return tree  \n",
    "        \n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wn.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('RB'): \n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "                \n",
    "def get_senses_for_sentence(current_sentence,scores):\n",
    "    word_sensed_tags = []\n",
    "    # for tuple in current sentence\n",
    "    for pos_tuple in current_sentence:\n",
    "        # Get the synsets of tuple\n",
    "        synsets = get_synsets(pos_tuple)\n",
    "        # If it's not none\n",
    "        if synsets is not None: \n",
    "            if len(synsets) != 0:\n",
    "                max_score = 0\n",
    "                max_synset = synsets[0]\n",
    "                for s in synsets:\n",
    "                    if scores[s] < 0 and scores[s] > max_score:\n",
    "                        max_score = scores[s]\n",
    "                        max_synset = s\n",
    "                word_sensed_tags.append((pos_tuple,max_synset))\n",
    "            else:\n",
    "                word_sensed_tags.append((pos_tuple,None))\n",
    "        else:\n",
    "            word_sensed_tags.append((pos_tuple,None))\n",
    "    return word_sensed_tags\n",
    "\n",
    "def get_target_word_replacement(senses,noun_sentence):\n",
    "    for sense_tuple in senses:\n",
    "        if sense_tuple[0][0] == noun_sentence[2]:\n",
    "            synset = sense_tuple[1]\n",
    "            return synset.lemmas()[0].name()\n",
    "                         \n",
    "def run_graph_word_sense(noun_sentences,pos_tags):\n",
    "    pos_tags = preprocess_pos_tags(pos_tags)\n",
    "    replacement_words = {}\n",
    "    for key in pos_tags:\n",
    "        current_sentence = pos_tags[key]\n",
    "        synset_graph = create_graph(current_sentence)\n",
    "        for synset in synset_graph:\n",
    "            tree = build_synset_tree(0,synset,[])\n",
    "            for s in tree:\n",
    "                if s in synset_graph.keys():\n",
    "                    edges = synset_graph[synset]\n",
    "                    if s not in edges:\n",
    "                        edges.append(s)\n",
    "                        synset_graph[synset] = edges\n",
    "                        edges = synset_graph[s]\n",
    "                        edges.append(synset)\n",
    "                        synset_graph[s] = edges\n",
    "        scores = {}\n",
    "        for synset in synset_graph:\n",
    "            degree = len(synset_graph[synset])/(len(synset_graph.keys())-1)\n",
    "            scores[synset] = degree\n",
    "        \n",
    "        word_senses = get_senses_for_sentence(current_sentence,scores)\n",
    "        replacement_words[key] = get_target_word_replacement(word_senses,noun_sentences[key])\n",
    "        print(replacement_words[key])\n",
    "    return replacement_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_in_gold_standard():\n",
    "    with open('assignment_resources/gold.trial') as f:\n",
    "        content = f.readlines()\n",
    "    content.pop(0)\n",
    "    content = [x.strip() for x in content]\n",
    "    return [x.split() for x in content]\n",
    "\n",
    "def get_noun_answers(content_array):\n",
    "    noun_sentences = {}\n",
    "    for evaluation in content_array:\n",
    "        given_tag = (evaluation[0].split(\".\"))[1]\n",
    "        if (given_tag is 'n'):\n",
    "            instance_id = evaluation[1]\n",
    "            noun_sentences[instance_id] = evaluation\n",
    "    return noun_sentences\n",
    "\n",
    "def create_answer_tuples(noun_dictionary):\n",
    "    for key in noun_dictionary:\n",
    "        # Gold standard row and sliced answers\n",
    "        gold_standard_row = noun_dictionary[key]\n",
    "        gold_standard_answers = gold_standard_row[3:len(gold_standard_row)]\n",
    "        noun_dictionary[key] = answer_list_to_tuples(gold_standard_answers)\n",
    "    return noun_dictionary\n",
    "\n",
    "def answer_list_to_tuples(gold_standard_answers):   \n",
    "        answer_tuples = []\n",
    "        i = 0\n",
    "        while i <(len(gold_standard_answers)):\n",
    "            if i is 0:\n",
    "                word = gold_standard_answers[i]\n",
    "            elif i is len(gold_standard_answers)-1:\n",
    "                break;\n",
    "            else:\n",
    "                word = (gold_standard_answers[i].split(';'))[1]\n",
    "            if check_if_tail_word(gold_standard_answers[i+1]):\n",
    "                index,end_of_tail_word = get_tail_words(i,gold_standard_answers)\n",
    "                word += end_of_tail_word\n",
    "                mark = gold_standard_answers[index].split(';')[0]\n",
    "                i = index\n",
    "            else:\n",
    "                mark = gold_standard_answers[i+1].split(';')[0]\n",
    "                i += 1\n",
    "            answer_tuples.append((word,mark))\n",
    "        return answer_tuples\n",
    "            \n",
    "def check_if_tail_word(possible_word):\n",
    "    return ';' not in possible_word\n",
    "\n",
    "def get_tail_words(current_index,answer_list):\n",
    "    found_last_tail = False\n",
    "    index = current_index + 1\n",
    "    tail_word = ''\n",
    "    while found_last_tail is not True:\n",
    "        if check_if_tail_word(answer_list[index]):\n",
    "            tail_word += (' '+ answer_list[index])\n",
    "            index +=1\n",
    "        else:\n",
    "            found_last_tail = True\n",
    "    return index, tail_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_replacements(replacement_dictionary,answer_dictionary):\n",
    "    scores = {}\n",
    "    max_score = 0\n",
    "    for key in replacement_dictionary:\n",
    "        replacement_word = replacement_dictionary[key]\n",
    "        if key in answer_dictionary.keys():\n",
    "            gold_standard_replacements = answer_dictionary[key]\n",
    "        else:\n",
    "            continue;\n",
    "        word_max_score = 0\n",
    "        for word,score in gold_standard_replacements:\n",
    "            if int(score) > word_max_score:\n",
    "                word_max_score = int(score)\n",
    "            if replacement_word == word:\n",
    "                scores[key] = int(score)\n",
    "        max_score += word_max_score\n",
    "        if key not in scores.keys():\n",
    "            scores[key] = 0\n",
    "    return max_score, scores \n",
    "\n",
    "\n",
    "def get_zero_scores(result_dictionary):\n",
    "    bad_scores = {}\n",
    "    for key in result_dictionary:\n",
    "        score = result_dictionary[key]\n",
    "        if score == 0:\n",
    "            bad_scores[key] = 0\n",
    "    return bad_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Training sentences\n",
    "    training_file_name = 'assignment_resources/lexsub_trial.xml'\n",
    "    root_of_training = get_root_of_training_file(training_file_name)\n",
    "    noun_sentences = get_noun_sentences_to_parse(root_of_training)\n",
    "    \n",
    "    # POS training sentences\n",
    "#     map_parsed_to_nouns = get_pos_for_sentences(noun_sentences)\n",
    "#     graph_replacement_dictionary = run_graph_word_sense(noun_sentences,map_parsed_to_nouns)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Training parsed sentences\n",
    "#     parsed_training_file_name = 'assignment_resources/lexsub_trial.parsed.xml'\n",
    "#     root_of_parsed_training = get_root_of_training_file(parsed_training_file_name)\n",
    "#     parsed_noun_sentences = get_parsed_noun_sentences(root_of_parsed_training,noun_sentences)\n",
    "    \n",
    "    \n",
    "    lesk_replacement_dictionary = create_lesk_replacement_dictionary(noun_sentences)\n",
    "    print(lesk_replacement_dictionary)\n",
    "    \n",
    "\n",
    "#     # Load Evaluation Set\n",
    "#     gold_standard_contents = load_in_gold_standard()\n",
    "#     noun_dictionary = get_noun_answers(gold_standard_contents)\n",
    "#     answer_dictionary = create_answer_tuples(noun_dictionary)\n",
    "    \n",
    "#     # Evaluate \n",
    "#     max_score, scores = score_replacements(replacement_dictionary,answer_dictionary)\n",
    "#     final_score = sum(scores.values())\n",
    "#     percentage = (final_score/max_score)*100\n",
    "#     poor_predictions = get_zero_scores(scores)\n",
    "\n",
    "#     print ('-'*40)\n",
    "#     print ('The max score that can be achieved for the sentences that were changed is: ',max_score)\n",
    "#     print ('-'*40)\n",
    "#     print ('The final score of the given replacements is:', final_score, '/', max_score , '(',percentage,'%)')\n",
    "#     print ('-'*40)\n",
    "#     print ('The following sentences were given a score of zero:')\n",
    "#     for key in poor_predictions:\n",
    "#         print ('-'*40)\n",
    "#         print(key ,': ',noun_sentences[key][0] + replacement_dictionary[key] + noun_sentences[key][1])\n",
    "#         print('The Gold Standard answers included:')\n",
    "#         for answer,mark in answer_dictionary[key]:\n",
    "#             print (answer,'-', mark)\n",
    "#         print('The original target word for this sentence was',noun_sentences[key][2])\n",
    "#         print('You decided that the best word to use was', replacement_dictionary[key])\n",
    "#         print ('-'*40)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11': 'plastic film', '12': 'movie', '13': 'movie', '14': 'movie', '15': 'movie', '16': 'movie', '17': 'movie', '18': 'movie', '19': 'movie', '20': 'movie', '43': 'legal profession', '44': 'legal profession', '45': 'legal profession', '47': 'legal profession', '49': 'legal profession', '50': 'Browning automatic rifle', '48': 'legal profession', '51': 'Cross', '52': 'Cross', '53': 'Cross', '54': 'crisscross', '55': 'hybridization', '56': 'Cross', '57': 'hybridization', '58': 'hybridization', '59': 'hybridization', '60': 'hybridization', '160': 'wilderness', '161': 'canful', '162': 'canful', '163': 'canful', '164': 'tin', '165': 'buttocks', '166': 'buttocks', '167': 'toilet', '168': 'canful', '169': 'toilet', '170': 'toilet', '171': 'night', '172': 'night', '173': 'night', '174': 'night', '175': 'night', '176': 'night', '177': 'night', '178': 'night', '179': 'night', '180': 'night', '181': 'examination', '182': 'examination', '183': 'interrogation', '184': 'examination', '185': 'examination', '186': 'examination', '187': 'examination', '188': 'examination', '189': 'examination', '190': 'examination', '211': 'bile', '212': 'resentment', '213': 'crust', '214': 'bile', '215': 'saddle sore', '216': 'crust', '217': 'crust', '218': 'saddle sore', '219': 'bile', '220': 'saddle sore'}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
