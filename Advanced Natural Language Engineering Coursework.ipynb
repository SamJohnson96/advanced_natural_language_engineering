{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wn_ic\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus import brown\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data extracting & parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets root of the xml file\n",
    "def get_root_of_training_file(training_file_name):\n",
    "    xml_file = ET.parse(training_file_name)\n",
    "    return xml_file.getroot()\n",
    "\n",
    "# Get all text of a given node. before and after target word.\n",
    "def get_node_text(node):\n",
    "    if node.text:\n",
    "        result = node.text\n",
    "    else:\n",
    "        result = ''\n",
    "    for child in node:\n",
    "        if child.tail is not None:\n",
    "            result += child.tail\n",
    "    return result\n",
    "    \n",
    "# Get all the sentences in the lexsub.xml file that are .n or have .n awarded by humans.\n",
    "def get_sentences_to_parse(root, pos_tag):\n",
    "    sentences = {}\n",
    "    for lexelt in root:\n",
    "        target = lexelt.attrib['item']\n",
    "        target = target.split('.')\n",
    "        target_tags = target[1:len(target)]\n",
    "        if len(target_tags) > 1:\n",
    "            target_tags = [target_tags[-1]];\n",
    "        if 'n' in target_tags:\n",
    "            target_word = target[0]\n",
    "            for instance in lexelt:\n",
    "                instance_id = instance.attrib['id']\n",
    "                for context in instance:\n",
    "                    sentence_text = []\n",
    "                    if context.text:\n",
    "                        sentence_text.append(context.text)\n",
    "                    else: \n",
    "                        sentence_text.append('')\n",
    "                    for child in context:\n",
    "                        sentence_text.append(child.tail)\n",
    "                        sentence_text.append(child.text)\n",
    "                sentences[instance_id] = sentence_text\n",
    "    return sentences\n",
    "        \n",
    "# Get PoS tags for every noun_sentence given\n",
    "def get_pos_for_sentences(noun_sentences):\n",
    "    parsed_sentences = {}\n",
    "    for key in noun_sentences:\n",
    "        sentence_to_find = noun_sentences[key][0] + noun_sentences[key][2] + noun_sentences[key][1]\n",
    "        parsed_sentences[key] = get_pos_tags(sentence_to_find)\n",
    "    return parsed_sentences\n",
    "\n",
    "# Get PoS tags for a given sentence using nltk package\n",
    "def get_pos_tags(sentence_to_find):                \n",
    "    tokenized = nltk.word_tokenize(sentence_to_find)\n",
    "    return nltk.pos_tag(tokenized)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to training file\n",
    "training_file_name = 'assignment_resources/lexsub_trial.xml'\n",
    "# Get the root of the training file .XML\n",
    "root_of_training = get_root_of_training_file(training_file_name)\n",
    "# Extract all sentences into a dictionary that replacement words are nouns. {id_of_sentence:[sentence,word_to_replace]}\n",
    "noun_sentences = get_noun_sentences_to_parse(root_of_training)\n",
    "# Get the POS tags for every word\n",
    "map_pos_to_nouns = get_pos_for_sentences(noun_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "print(len(noun_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a stemmer on the target word\n",
    "def stem_target_word(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemword = stemmer.stem(word)\n",
    "    if wn.synsets(stemword) is None:\n",
    "        stemword = word\n",
    "    return stemword\n",
    "\n",
    "# Return sentence in lowercase\n",
    "def sentences_to_lowercase(sentence):\n",
    "    return ([segment.lower() for segment in sentence])\n",
    "\n",
    "# Preprocess a given sentence with it stemmed target word and lowercase sentence.\n",
    "def preprocess_sentence(sentence):\n",
    "    lowercase_sentence = sentences_to_lowercase(sentence)\n",
    "    stemmed_target_word = stem_target_word(lowercase_sentence[2])\n",
    "    return lowercase_sentence,stemmed_target_word\n",
    "\n",
    "# Preprocess post tag sentences. This removes all stop words from the sentence as well as punctuation.\n",
    "def preprocess_pos_tags(pos_tags):\n",
    "    processed_tags = {}\n",
    "    for key in pos_tags:\n",
    "        current_sentence = pos_tags[key]\n",
    "        text_sentence = [(sentence[0].lower(),sentence[1]) for sentence in current_sentence]\n",
    "        stopwords_removed =  [(w[0],w[1]) for w in text_sentence if not w[0] in stopwords.words('english')]\n",
    "        punctuation = ['!',',','.','?',')','(']\n",
    "        punctuation_removed = [(w[0],w[1]) for w in stopwords_removed if not w[0] in punctuation]\n",
    "        processed_tags[key] = punctuation_removed\n",
    "    return processed_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all stop words, punctuation for graphs and \n",
    "preprocessed_pos_tags = preprocess_pos_tags(map_pos_to_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word sense disambiguation using measures of word semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senses_of_target_words(max_reach=3):\n",
    "    # Replacement dictionary\n",
    "    replacement_dict = {}\n",
    "    # For every sentence that has the target word as a noun\n",
    "    for sentence in noun_sentences:\n",
    "        # Get the current sentence\n",
    "        current_sentence = noun_sentences[sentence]\n",
    "        # Create array of words that the sentence is made of\n",
    "        tokenised_sentence = tokenise_sentence([current_sentence[0],current_sentence[2],current_sentence[1]])\n",
    "        # Create graph of sentence\n",
    "        graph = create_sentence_graph(tokenised_sentence)\n",
    "                \n",
    "        # For every entry in dictionary\n",
    "        for i in range(0, len(graph)):\n",
    "            # Get current word\n",
    "            current_word = graph[i]\n",
    "            # For every synset in current words dictionary\n",
    "            for synset in current_word:\n",
    "                # Get range of indexes to search using max_reach parameter\n",
    "                indexes_to_search = get_ranges(max_reach,i)\n",
    "                # For every dictionary that is not this word and in the max reach\n",
    "                for word_index in indexes_to_search:\n",
    "                    # If the index exists in the sentence\n",
    "                    if word_index >= 0 and word_index < len(graph):\n",
    "                        # Get the word we're looking at dictionary\n",
    "                        check_word_dictionary = graph[word_index]\n",
    "                        # For synset in it's dictionary\n",
    "                        for similarity_synset in check_word_dictionary:\n",
    "                            # Check to see if synset exists in graph edges already\n",
    "                            if not similarity_already_measured(current_word[synset],similarity_synset):                                \n",
    "                                # Calculate similarity\n",
    "                                sim = synset.wup_similarity(similarity_synset)\n",
    "                                # If it's not none\n",
    "                                if sim is not None:\n",
    "                                    # Store in the current words edges\n",
    "                                    current_word[synset].append((sim,word_index,similarity_synset))\n",
    "                                    # Store in the synset we found\n",
    "                                    check_word_dictionary[similarity_synset].append((sim,i,synset))\n",
    "        # Score each label\n",
    "        scored_graph = score_graph(graph)\n",
    "        \n",
    "        # Select max label:\n",
    "        max_graph = sense_sentence(scored_graph)\n",
    "        \n",
    "        # Get sense of target word:\n",
    "        target_word = target_word_sense(current_sentence,max_graph)\n",
    "        \n",
    "        # Assign to dictionary with sentence id as key\n",
    "        replacement_dict[sentence] = target_word\n",
    "        \n",
    "    return replacement_dict           \n",
    "                    \n",
    "def tokenise_sentence(sentence_array):\n",
    "    tokenised_sentence = []\n",
    "    for split in sentence_array:\n",
    "        tokenised_sentence.append(word_tokenize(split))\n",
    "    return [item for sublist in tokenised_sentence for item in sublist]\n",
    "\n",
    "def create_sentence_graph(tokenised_sentence):\n",
    "    # Create a dictionary where each entry is a word to access its labels\n",
    "    sentence_dictionary = []\n",
    "    # For 0 to the end of the tokenised sentence\n",
    "    for i in range(0, len(tokenised_sentence)-1):\n",
    "        # Every word dictionary item will hold a dictionary filled with it's synsets which are VERTICES\n",
    "        sentence_dictionary.append({})\n",
    "        # Get word synsets\n",
    "        word_synsets = wn.synsets(tokenised_sentence[i])\n",
    "        # For every synset\n",
    "        for synset in word_synsets:\n",
    "            # Assign the synset dictionary as 0 \n",
    "            sentence_dictionary[i][synset] = []\n",
    "    # Return full sentence dictionary\n",
    "    return sentence_dictionary\n",
    "    \n",
    "def get_ranges(max_reach,index):\n",
    "    # Get previous indexes\n",
    "    start = index - max_reach\n",
    "    # Get future indexes\n",
    "    finish = index + max_reach\n",
    "    # Get range and turn it into array\n",
    "    range_of_numbers = list(range(start,finish))\n",
    "    # Flatten array\n",
    "    return [x for x in range_of_numbers if x != index]\n",
    "\n",
    "def similarity_already_measured(synset_dictionary, current_synset):\n",
    "    # for each array in dictionary entry\n",
    "    for tup in synset_dictionary:\n",
    "        # If the tuple already contains the synset that we're checking\n",
    "        if tup[2] == current_synset:\n",
    "            # Return True\n",
    "            return True;\n",
    "    # If synset doesn't exist, return True\n",
    "    return False\n",
    "    \n",
    "def score_graph(graph):\n",
    "    # New graph to return\n",
    "    scored_graph = []\n",
    "    # For every word in graph\n",
    "    for word in graph:\n",
    "        # Create a new dictionary\n",
    "        scored_synsets = {}\n",
    "        # For every synset in word\n",
    "        for synset in word:\n",
    "            # Get the contents of the dictionary entry\n",
    "            edges = word[synset]\n",
    "            # Init empty score\n",
    "            edge_degree = 0\n",
    "            # For tuple in the dictionary entry\n",
    "            for tup in edges:\n",
    "                # Add the score to edge degree\n",
    "                edge_degree += tup[0]\n",
    "            # Reassign the value in the dictionary to it's degree central\n",
    "            scored_synsets[synset] = edge_degree \n",
    "        # Append word with scores to graph array\n",
    "        scored_graph.append(scored_synsets)\n",
    "    # Return new scored graph\n",
    "    return scored_graph\n",
    "        \n",
    "\n",
    "def sense_sentence(scored_graph):\n",
    "    # What will be the final sentence synsets\n",
    "    sentence = []\n",
    "    # For every word in the graph\n",
    "    for word in scored_graph:\n",
    "        try:\n",
    "            # Assign current best to the first\n",
    "            best_word = list(word.keys())[0]\n",
    "            # Best score to 0\n",
    "            best_score = 0\n",
    "        except:\n",
    "            # None type for best word as it has no synsets\n",
    "            best_word = None\n",
    "            # Best score is 0\n",
    "            best_score = 0\n",
    "        # For every synset in words dictionary\n",
    "        for synset in word:\n",
    "            if word[synset] > best_score:\n",
    "                # Set best word to new word\n",
    "                best_word = synset\n",
    "                # Set best score to new score\n",
    "                best_score = word[synset]\n",
    "        # Append to sense sentence\n",
    "        sentence.append(best_word)\n",
    "    # Return new sentence\n",
    "    return sentence\n",
    "        \n",
    "def target_word_sense(current_sentence, sentence_graph):\n",
    "    # Get the position of the target word.\n",
    "    before_word = len(word_tokenize(current_sentence[0]))\n",
    "    return sentence_graph[before_word]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1f6193ec5151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph_word_sense_disambiguation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_senses_of_target_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-236585ead459>\u001b[0m in \u001b[0;36mget_senses_of_target_words\u001b[0;34m(max_reach)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtokenised_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenise_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_sentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_sentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_sentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Create graph of sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sentence_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenised_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# For every entry in dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-236585ead459>\u001b[0m in \u001b[0;36mcreate_sentence_graph\u001b[0;34m(tokenised_sentence)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0msentence_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Get word synsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mword_synsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenised_sentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;31m# For every synset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_synsets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reader_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# This is where the magic happens!  Transform ourselves into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;31m# Load the indices for lemmas and synset offsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_lemma_pos_offset_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# load the exception file data into memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_load_lemma_pos_offset_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;31m# parse each line of the file (ignoring comment lines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index.%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0;34m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinebuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m                 self._rewind_numchars = (len(new_chars) -\n\u001b[1;32m   1181\u001b[0m                                          (len(chars) - len(line)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph_word_sense_disambiguation = get_senses_of_target_words(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in graph_word_sense_disambiguation:\n",
    "    print(graph_word_sense_disambiguation[key])\n",
    "print(len(graph_word_sense_disambiguation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target word dictionary\n",
    "def get_target_word_replacement():\n",
    "    replacements = {}\n",
    "    # For every sentence thats sense has contents classified with sense.\n",
    "    for key in graph_word_sense_disambiguation:\n",
    "        # Get the guessed synset\n",
    "        current_guessed_synset = graph_word_sense_disambiguation[key]\n",
    "        # Get the current noun sentence\n",
    "        current_trial_sentence = noun_sentences[key]  \n",
    "        # Out of ten array\n",
    "        oot_array = []\n",
    "        \n",
    "        #### Get synset word ####\n",
    "        synset_name = get_synset_name(current_guessed_synset,current_trial_sentence)\n",
    "        \n",
    "        #### Get lemmas of synset ####\n",
    "        lemmas = get_lemmas(current_guessed_synset, current_trial_sentence[2])\n",
    "        \n",
    "        #### Get holonyms of synset ####\n",
    "        holonyms = get_part_holonyms(current_guessed_synset, current_trial_sentence[2])\n",
    "        \n",
    "        #### Get lesk replacement word ####\n",
    "        sentence = current_trial_sentence[0] + current_trial_sentence[2] + current_trial_sentence[1]\n",
    "        lesk_replacement = get_synset_name(lesk(sentence,current_trial_sentence[2],'n'),current_trial_sentence[2])\n",
    "                \n",
    "        # Create an array out of 10 #\n",
    "        oot_array = create_oot_array([synset_name,lemmas,holonyms,lesk_replacement],current_trial_sentence[2])\n",
    "                \n",
    "        replacements[key] = oot_array\n",
    "    return replacements\n",
    "\n",
    "\n",
    "def get_synset_name(synset,word_to_replace):\n",
    "    return [synset.name().split('.')[0]]\n",
    "\n",
    "\n",
    "def get_lemmas(synset,word_to_replace):\n",
    "    lemmas = []\n",
    "    # Go through lemmas\n",
    "    for lemma in synset.lemmas():\n",
    "        lemmas.append(lemma.name())\n",
    "    return lemmas\n",
    "\n",
    "def get_part_holonyms(synset,word_to_replace):\n",
    "    # Go through holonyms\n",
    "    part_holonyms = []\n",
    "    for holonym in synset.part_holonyms():\n",
    "        part_holonyms = [lem.name() for lem in holonym.lemmas()]\n",
    "    return part_holonyms\n",
    "\n",
    "def create_oot_array(replacements,word_to_replace):\n",
    "    # Out of ten array\n",
    "    oot = []\n",
    "    # For each list of replacements\n",
    "    for replacement_list in replacements:\n",
    "        # For each word in list\n",
    "        for replacement_word in replacement_list:\n",
    "            # If it's not equal to the word to replace\n",
    "            if replacement_word != word_to_replace.lower():\n",
    "                # And not equal to the stemmed version\n",
    "                if stem_target_word(word_to_replace.lower()) != stem_target_word(replacement_word):\n",
    "                    # If there is an underscore -- sometimes the case\n",
    "                    if '_' in replacement_word:\n",
    "                        # Split the word by underscores\n",
    "                        word = replacement_word.split('_')\n",
    "                        # Create new word with spaces\n",
    "                        split_word = ''\n",
    "                        for split in word:\n",
    "                            split_word += split + ' '\n",
    "                        replacement_word = split_word\n",
    "                    # If it's not in there then add it   \n",
    "                    if replacement_word not in oot:\n",
    "                        oot.append(replacement_word)\n",
    "    oot.append('hello')\n",
    "    return oot\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement dictionary\n",
    "replacement_dictionary = get_target_word_replacement()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open gold standard file that contains the annotators answers\n",
    "def load_in_gold_standard():\n",
    "    with open('assignment_resources/gold.trial') as f:\n",
    "        content = f.readlines()\n",
    "    content.pop(0)\n",
    "    content = [x.strip() for x in content]\n",
    "    return [x.split() for x in content]\n",
    "\n",
    "# Get the noun sentences from the gold standard file\n",
    "def get_noun_answers(content_array):\n",
    "    noun_sentences = {}\n",
    "    for evaluation in content_array:\n",
    "        split_eval = (evaluation[0].split(\".\"))\n",
    "        if (len(split_eval) > 2):\n",
    "            given_tag = split_eval[-1]\n",
    "        else:\n",
    "            given_tag = split_eval[1]\n",
    "        if (given_tag is 'n'):\n",
    "            instance_id = evaluation[1]\n",
    "            noun_sentences[instance_id] = evaluation\n",
    "    return noun_sentences\n",
    "\n",
    "# Go through the gold standard noun sentences and trn scores into a tuple\n",
    "def create_answer_tuples(noun_dictionary):\n",
    "    for key in noun_dictionary:\n",
    "        # Gold standard row and sliced answers\n",
    "        gold_standard_row = noun_dictionary[key]\n",
    "        gold_standard_answers = gold_standard_row[3:len(gold_standard_row)]\n",
    "        noun_dictionary[key] = answer_list_to_tuples(gold_standard_answers)\n",
    "    return noun_dictionary\n",
    "\n",
    "# Gold standard sentence to tuple with scores\n",
    "def answer_list_to_tuples(gold_standard_answers):   \n",
    "        answer_tuples = []\n",
    "        i = 0\n",
    "        while i <(len(gold_standard_answers)):\n",
    "            if i is 0:\n",
    "                word = gold_standard_answers[i]\n",
    "            elif i is len(gold_standard_answers)-1:\n",
    "                break;\n",
    "            else:\n",
    "                word = (gold_standard_answers[i].split(';'))[1]\n",
    "            if check_if_tail_word(gold_standard_answers[i+1]):\n",
    "                index,end_of_tail_word = get_tail_words(i,gold_standard_answers)\n",
    "                word += end_of_tail_word\n",
    "                mark = gold_standard_answers[index].split(';')[0]\n",
    "                i = index\n",
    "            else:\n",
    "                mark = gold_standard_answers[i+1].split(';')[0]\n",
    "                i += 1\n",
    "            answer_tuples.append((word,mark))\n",
    "        return answer_tuples\n",
    "            \n",
    "# Used in answer_list_to_tuples function\n",
    "def check_if_tail_word(possible_word):\n",
    "    return ';' not in possible_word\n",
    "\n",
    "# Used in answer_list_to_tuples function\n",
    "def get_tail_words(current_index,answer_list):\n",
    "    found_last_tail = False\n",
    "    index = current_index + 1\n",
    "    tail_word = ''\n",
    "    while found_last_tail is not True:\n",
    "        if check_if_tail_word(answer_list[index]):\n",
    "            tail_word += (' '+ answer_list[index])\n",
    "            index +=1\n",
    "        else:\n",
    "            found_last_tail = True\n",
    "    return index, tail_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score each replacement - Total score\n",
    "def score_replacements():\n",
    "    # Score given\n",
    "    scores = {}\n",
    "    # Total max score able to be achieved\n",
    "    max_score = 0\n",
    "    # For replacement offered in replacement ditionary\n",
    "    for key in replacement_dictionary:\n",
    "        \n",
    "        # Get the ID that we're going to look at\n",
    "        replacement_words = replacement_dictionary[key]\n",
    "        \n",
    "        # Get the gold standard id\n",
    "        if key in answer_dictionary.keys():\n",
    "            gold_standard_replacements = answer_dictionary[key]\n",
    "        else:\n",
    "            continue;\n",
    "            \n",
    "        # Word max score\n",
    "        word_max_score = 0\n",
    "        current_score = 0\n",
    "        \n",
    "        # For tuple in gold standard answer row\n",
    "        for word,score in gold_standard_replacements:\n",
    "            # If score is bigger than the current word max score\n",
    "            if int(score) > word_max_score:\n",
    "                # REPLACE\n",
    "                word_max_score = int(score)\n",
    "            # For each replacement offered\n",
    "            for replacement in replacement_words:\n",
    "                if replacement == word and int(score) > current_score:\n",
    "                    current_score = int(score)\n",
    "        max_score += word_max_score\n",
    "        scores[key] = current_score\n",
    "        \n",
    "    return max_score, scores \n",
    "\n",
    "# Score precision - Using evaluation method stated in paper\n",
    "def score_precision(replacement_dictionary,answer_dictionary):\n",
    "    suggestion_precision = 0\n",
    "    for noun_id in answer_dictionary:\n",
    "        gold_standard_tuples = answer_dictionary[noun_id]\n",
    "        # Calculate total possible score\n",
    "        total_possible_score = 0\n",
    "        for tup in gold_standard_tuples:\n",
    "            total_possible_score += int(tup[1])            \n",
    "        # Calculate the score given to the suggestions    \n",
    "        suggestions = replacement_dictionary[noun_id]\n",
    "        total_score = 0\n",
    "        for replacement in suggestions:\n",
    "            for gold_answer in gold_standard_tuples:\n",
    "                if gold_answer[0] == replacement:\n",
    "                    total_score += int(gold_answer[1])\n",
    "    \n",
    "        suggestion_precision += total_score / total_possible_score\n",
    "    return suggestion_precision / len(answer_dictionary)\n",
    "\n",
    "# Score recall - Using evaluation method stated in paper\n",
    "def score_recall(replacement_dictionary,answer_dictionary):\n",
    "    total_recall = 0\n",
    "    for noun_id in answer_dictionary:\n",
    "        gold_standard_tuples = answer_dictionary[noun_id]\n",
    "        # Calculate total possible score\n",
    "        total_possible_score = 0\n",
    "        for tup in gold_standard_tuples:\n",
    "            total_possible_score += int(tup[1])\n",
    "    return suggestion_precision / len(answer_dictionary)\n",
    "    \n",
    "\n",
    "def get_zero_scores(result_dictionary):\n",
    "    bad_scores = {}\n",
    "    for key in result_dictionary:\n",
    "        score = result_dictionary[key]\n",
    "        if score == 0:\n",
    "            bad_scores[key] = 0\n",
    "    return bad_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gold standard file\n",
    "gold_standard_contents = load_in_gold_standard()\n",
    "# Create a dictionary with only the answers with nouns\n",
    "noun_dictionary = get_noun_answers(gold_standard_contents)\n",
    "# Process the dictionary into an easily readable answers\n",
    "answer_dictionary = create_answer_tuples(noun_dictionary)\n",
    "\n",
    "# Get precision score\n",
    "# precision = score_precision(replacement_dictionary, answer_dictionary)\n",
    "# print(precision)\n",
    "\n",
    "# Get recall score\n",
    "# recall = score_recall(answer_dictionary)\n",
    "# print(recall)\n",
    "\n",
    "# Get total score evaluation\n",
    "max_score, scores = score_replacements()\n",
    "final_score = sum(scores.values())\n",
    "percentage = (final_score/max_score)*100\n",
    "poor_predictions = get_zero_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The max score that can be achieved for the sentences that were changed is: ',max_score)\n",
    "print ('-'*40)\n",
    "print ('The final score of the given replacements is:', final_score, '/', max_score , '(',percentage,'%)')\n",
    "print ('-'*40)\n",
    "print ('The following sentences were given a score of zero:')\n",
    "for key in poor_predictions:\n",
    "    print ('-'*40)\n",
    "    if replacement_dictionary[key] != None:\n",
    "        print(key ,': ',noun_sentences[key][0] + noun_sentences[key][2] + noun_sentences[key][1])\n",
    "        print(\"\\n\")\n",
    "        print('You gave the following responses:')\n",
    "        print ('-'*20)\n",
    "        for word in replacement_dictionary[key]:\n",
    "            print(word)\n",
    "    else:\n",
    "        print('You could not find a suitable prediction for this sentence and returned None') \n",
    "    print ('-'*80)\n",
    "    print('The Gold Standard answers included:')\n",
    "    for answer,mark in answer_dictionary[key]:\n",
    "        print (answer,'-', mark)\n",
    "#     print('The original target word for this sentence was',noun_sentences[key][2])\n",
    "#     print('You decided that the best word to use was', replacement_dictionary[key])\n",
    "#     print ('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse whole sentence into segments\n",
    "def parse_sentence_to_array(sentence):\n",
    "    return (sentence[0] + sentence[2] + sentence[1]).split()\n",
    "\n",
    "# Run the lesk algorithm on a given sentence and target word\n",
    "def perform_lesk(sentence,word):\n",
    "    return lesk(sentence,word,'n')\n",
    "\n",
    "# Create a replacement dictionary for lesk.\n",
    "def create_lesk_replacement_dictionary(noun_sentences):\n",
    "    replacement_dictionary = {}\n",
    "    # For every sentence\n",
    "    for sentence in noun_sentences:\n",
    "        # place sentence to lowercase and stem the target word. \n",
    "        preprocessed_sentence,target_word = preprocess_sentence(noun_sentences[sentence])     \n",
    "        # Run the lesk algorithm to get the WSD and sense of the target word.\n",
    "        context_synset = perform_lesk(parse_sentence_to_array(preprocessed_sentence),target_word)\n",
    "        \n",
    "        # this is to catch any errors from the preprocessing and make sure we get an answer back\n",
    "        if context_synset is None:\n",
    "            context_synset = perform_lesk(parse_sentence_to_array(preprocessed_sentence),noun_sentences[sentence][2])\n",
    "            \n",
    "        # For every lemma on the word sense\n",
    "        for lemma in context_synset.lemmas():\n",
    "            # If it doesn't equal the target word already\n",
    "            if target_word != lemma.name():\n",
    "                # Get the replacement word\n",
    "                replacement_word = lemma.name()\n",
    "                # Check if there is a split in the sentence\n",
    "                if '_' in replacement_word:\n",
    "                    split_word = ''\n",
    "                    for word in replacement_word.split('_'):\n",
    "                        split_word += ' ' + word\n",
    "                    replacement_word = split_word[1:]\n",
    "                # Set the dictionary sentence id to equal the replacement.\n",
    "                replacement_dictionary[sentence] = replacement_word\n",
    "                break;\n",
    "    return replacement_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph_word_sense(noun_sentences,max_level):\n",
    "    print('--- Starting ---')\n",
    "    word_sensed_dictionary = {}\n",
    "    # Make a copy of the preprocessed_pos_tags\n",
    "    pos_tags = preprocessed_pos_tags\n",
    "    # For every sentence in the sentences\n",
    "    for key in pos_tags:\n",
    "        print('--- Getting sentence %d and creating tree ---', (%key))\n",
    "        # Get current sentence\n",
    "        current_sentence = pos_tags[key]\n",
    "        # Create a graph with all synsets that are in the sentence.\n",
    "        synset_graph = create_graph(current_sentence)\n",
    "        # For each node(synset) in the graph\n",
    "        for synset in synset_graph:\n",
    "            # Build a tree getting all it's lexical relations\n",
    "            tree = build_synset_tree(0,synset,[],max_level)\n",
    "            # For every relation in the tree found\n",
    "            for relation in tree:\n",
    "                # If it is fond within the graph and it's not referring to itself\n",
    "                if relation in synset_graph.keys() and relation is not synset:\n",
    "                    # Get the array value in the dictionary (list of edges)\n",
    "                    edges = synset_graph[synset]\n",
    "                    # If the current relation isn't in the nodes edges\n",
    "                    if relation not in edges:\n",
    "                        # Append edge to the current synset which goes to the relation found in the graph\n",
    "                        edges.append(relation)\n",
    "                        # Assign new edges to graph\n",
    "                        synset_graph[synset] = edges\n",
    "                        # Get the edges of whatever the relation found was\n",
    "                        edges = synset_graph[relation]\n",
    "                        # Replicate edge in its list\n",
    "                        edges.append(synset)\n",
    "                        # Assign edge\n",
    "                        synset_graph[relation] = edges\n",
    "                        \n",
    "        # Now the graphs have a list of edges, create a dictionary of scores.\n",
    "        scores = {}\n",
    "        # For every synset in graph\n",
    "        for synset in synset_graph:\n",
    "            # Get the degree of its edges\n",
    "            degree = len(synset_graph[synset])/(len(synset_graph.keys())-1)\n",
    "            # Add to score\n",
    "            scores[synset] = degree\n",
    "        \n",
    "        # Get the largest score for each synset\n",
    "        word_senses = get_senses_for_sentence(current_sentence,noun_sentences[key],scores)\n",
    "        word_sensed_dictionary[key] = word_senses\n",
    "        print('--- Word sense has been created ---')\n",
    "    return word_sensed_dictionary\n",
    "\n",
    "\n",
    "# Create a dictionary graph for every synset within the sentence.\n",
    "def create_graph(sentence):\n",
    "    graph = {}\n",
    "    for pos_tuple in sentence:\n",
    "        synsets = get_synsets(pos_tuple)\n",
    "        if synsets is not None:\n",
    "            for s in synsets:\n",
    "                graph[s] = [];\n",
    "    return graph  \n",
    "\n",
    "# Get all synsets based on the POS tag and given word\n",
    "def get_synsets(pos_tuple):\n",
    "    wn_pos_code = wordnet_pos_code(pos_tuple[1])\n",
    "    if wn_pos_code is not None:\n",
    "        return wn.synsets(pos_tuple[0],wn_pos_code)\n",
    "\n",
    "# Map POS tags to wordnet tags.\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wn.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('RB'): \n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "                    \n",
    "\n",
    "# Given a synset, recursively build a tree based on its lexical and semantic relations .\n",
    "def build_synset_tree(level,synset,tree,max_level):\n",
    "    if synset not in tree and level != 0:\n",
    "        tree.append(synset)\n",
    "    if level <= max_level:\n",
    "        # Create trees of all lexical relations.\n",
    "        \n",
    "        # Get Hyponyms\n",
    "        for s in synset.hyponyms():\n",
    "            build_synset_tree(level+1,s,tree,max_level)\n",
    "            \n",
    "        # Get Hypernyms\n",
    "        for s in synset.hypernyms():\n",
    "            build_synset_tree(level+1,s,tree,max_level)\n",
    "            \n",
    "        # Get Holonyms\n",
    "        for s in synset.member_holonyms():\n",
    "            build_synset_tree(level+1,s,tree,max_level)\n",
    "            \n",
    "        # Get Meronyms\n",
    "        for s in synset.part_meronyms():\n",
    "            build_synset_tree(level+1,s,tree,max_level)\n",
    "            \n",
    "        # Get Norminalisations\n",
    "        for l in synset.lemmas():\n",
    "            related_forms = l.derivationally_related_forms()\n",
    "            for rf in related_forms:\n",
    "                build_synset_tree(level+1,rf.synset(),tree,max_level) \n",
    "                \n",
    "        return tree\n",
    "    else:\n",
    "        return tree  \n",
    "        \n",
    "# With all scores being calculated, chose the highest scoring senses, if 0 then pick first.\n",
    "def get_senses_for_sentence(current_sentence,noun_sentences,scores):\n",
    "    word_sensed_tags = []\n",
    "    # For tuple in current sentence\n",
    "    for pos_tuple in current_sentence:\n",
    "        # Get the synsets of each tuple\n",
    "        synsets = get_synsets(pos_tuple)\n",
    "        # if it's not none\n",
    "        if synsets is not None: \n",
    "            # code slip up, sometimes comes as zero.\n",
    "            if len(synsets) != 0:\n",
    "                # Keep track of what the max score is.\n",
    "                max_score = 0\n",
    "                # Set the current max synset to the first\n",
    "                max_synset = synsets[0]\n",
    "                # For every synset for the current word\n",
    "                for s in synsets:\n",
    "                    # If score is bigger than zero and the score of the current synset is bigger than the current max\n",
    "                    if scores[s] < 0 and scores[s] > max_score:\n",
    "                        # Assign new max score\n",
    "                        max_score = scores[s]\n",
    "                        # Assign new synset are the current word sense.\n",
    "                        max_synset = s\n",
    "                # Create a tuple, and give the current word alongside the synset sense it has been rewarded.\n",
    "                word_sensed_tags.append((pos_tuple,max_synset))\n",
    "            else:\n",
    "                # If synsets are zero then give back None. We're not doing it like this.\n",
    "                word_sensed_tags.append((pos_tuple,None))\n",
    "        else:\n",
    "            # If no synsets then give back None. We're not doing it like this. \n",
    "            word_sensed_tags.append((pos_tuple,None))\n",
    "    # Return back array of tuples with word senses attached to them.\n",
    "    return word_sensed_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_word_sense_disambiguation = run_graph_word_sense(noun_sentences,3) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
